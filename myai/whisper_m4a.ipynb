{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aehyok/go-openai/blob/main/%E2%80%9Cwhisper_m4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73JHCOCS99hl"
      },
      "source": [
        "直接将m4a音频文件转换为字幕文件srt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBUm1pmC90Mk",
        "outputId": "edd6feb2-8c38-4164-b4b8-d624952c2b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-30297e83-80fd-9354-3339-12897273eb98)\n",
            "Mon Mar 27 11:44:21 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **检查GPU硬件加速** 🕵️\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMRtV2Lw_BHU",
        "outputId": "5b7ec378-41f6-4147-d34f-496e9f5c4cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4) (2.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-6p9at6gc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-6p9at6gc\n",
            "  Resolved https://github.com/openai/whisper.git to commit 6dea21fd7f7253bfe450f1e2512a0fe47ee2d258\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.13.1+cu116)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: tiktoken==0.3.1 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.3.1)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.18.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.10.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (67.6.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.15)\n",
            "Whisper已经被安装请执行下一个单元\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "正在使用的设备: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown **配置Whisper/Setup Whisper** 🏗️\n",
        "\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('正在使用的设备:', device, file=sys.stderr)\n",
        "\n",
        "print('Whisper已经被安装请执行下一个单元')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "nPV9yHeqARiK",
        "outputId": "0d4e2455-b66e-4e86-eb73-e84c67ba841e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**large-v2 model已经被选择了**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @markdown # ** whisper Model选择 ** 🧠\n",
        "\n",
        "\n",
        "Model = 'large-v2' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large', 'large-v2']\n",
        "#@markdown ---\n",
        "\n",
        "import whisper\n",
        "from IPython.display import Markdown\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model已经被选择了**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.** Please select one of the following: - {' - '.join(whisper.available_models())}\"\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lOnyZpK2EsVe",
        "outputId": "c4ac1e70-4219-4cbc-9450-61e98d91e768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "audio local path: /content/downloads/test1.m4a\n",
            "audio audio_path: /content/downloads/test1.m4a\n",
            "[00:00.000 --> 00:04.100] 你好,我是咱们专栏的学习委员朱英达\n",
            "[00:04.100 --> 00:06.500] 在预备知识片这个模块呢\n",
            "[00:06.500 --> 00:08.700] 龚老师给我们系统地梳理了\n",
            "[00:08.700 --> 00:11.500] 编译过程中各个阶段的核心要点\n",
            "[00:11.500 --> 00:12.800] 主要的目的呢\n",
            "[00:12.800 --> 00:16.500] 就是让我们建立一个编译原理的知识体系\n",
            "[00:16.500 --> 00:18.300] 那么到今天为止呢\n",
            "[00:18.300 --> 00:20.500] 我们就学完这部分内容了\n",
            "[00:20.500 --> 00:23.700] 那你对这些知识掌握得如何呢\n",
            "[00:23.700 --> 00:27.300] 为了复习,也为了检测我们的学习成果\n",
            "[00:27.500 --> 00:30.600] 我就根据自己的积累和一些学习经验\n",
            "[00:30.600 --> 00:32.800] 整理了一张知识地图\n",
            "[00:32.800 --> 00:37.100] 你呢,可以根据这张地图中标记的七大编译阶段\n",
            "[00:37.100 --> 00:38.800] 用来随时速查那些\n",
            "[00:38.800 --> 00:42.700] 我们经常使用的编译原理概念和关键算法\n",
            "[00:42.700 --> 00:45.700] 如果你也总结了自己的知识地图呢\n",
            "[00:45.700 --> 00:48.200] 那你也可以对照着我整理的这个\n",
            "[00:48.200 --> 00:49.700] 给自己一个反馈\n",
            "[00:49.700 --> 00:52.000] 看看它们之间有哪些一样\n",
            "[00:52.000 --> 00:53.800] 或者是不一样的地方\n",
            "[00:53.800 --> 00:56.700] 我们可以在留言区一起进行交流\n",
            "[00:56.700 --> 00:59.300] 不过啊,虽然知识地图的形式\n",
            "[00:59.300 --> 01:02.100] 比较方便你保存、携带和速查\n",
            "[01:02.100 --> 01:06.300] 但是考虑到知识地图里涉及的一些理论啊、概念啊\n",
            "[01:06.300 --> 01:08.400] 这些知识内容会比较多\n",
            "[01:08.400 --> 01:10.700] 不方便你进行查找检索\n",
            "[01:10.700 --> 01:14.600] 所以,我还特意把地图上的一些知识点\n",
            "[01:14.600 --> 01:17.700] 用文字的形式帮你进行了梳理\n",
            "[01:17.700 --> 01:19.700] 你可以对照着文字内容\n",
            "[01:19.700 --> 01:23.900] 复习和回顾编译技术的核心概念和关键算法\n",
            "[01:23.900 --> 01:26.700] 用来构建属于自己的知识框架\n",
            "[01:26.700 --> 01:29.900] 另外,你在学习这些预备知识的过程中呢\n",
            "[01:29.900 --> 01:31.100] 可能会发现\n",
            "[01:31.100 --> 01:32.900] 龚老师并没有非常深入地\n",
            "[01:32.900 --> 01:36.700] 去讲解这些编译原理的具体概念、理论和算法\n",
            "[01:36.700 --> 01:38.900] 所以啊,如果你想继续深入地\n",
            "[01:38.900 --> 01:40.900] 去学习这些基础知识\n",
            "[01:40.900 --> 01:43.300] 那你可以根据龚老师在每一讲最后\n",
            "[01:43.300 --> 01:44.900] 列出的参考资料\n",
            "[01:44.900 --> 01:47.900] 像龙书、虎书、经书等等\n",
            "[01:47.900 --> 01:50.700] 这些比较经典的理论书籍\n",
            "[01:50.700 --> 01:51.500] 当然了\n",
            "[01:51.500 --> 01:54.500] 你也可以去看看龚老师的第一季专栏课\n",
            "[01:54.500 --> 01:56.300] 编译原理之美\n",
            "[01:56.300 --> 01:57.500] 在我看来啊\n",
            "[01:57.500 --> 02:00.500] 相比编译方面的一些教科书来说呢\n",
            "[02:00.500 --> 02:03.700] 编译原理之美这门课会更加通俗易懂\n",
            "[02:03.700 --> 02:04.900] 与时俱进\n",
            "[02:04.900 --> 02:07.700] 它既可以作为一个新手的起步指导\n",
            "[02:07.700 --> 02:09.100] 也能帮助一些\n",
            "[02:09.100 --> 02:12.500] 比较熟悉编译技术的工程师来扩展视野\n",
            "[02:12.500 --> 02:15.300] 我呢,很推荐你去学习这门课\n",
            "[02:15.300 --> 02:18.100] 所以啊,我邀请了咱们的专栏编辑\n",
            "[02:18.100 --> 02:19.700] 添加了相应的知识点\n",
            "[02:19.700 --> 02:22.500] 到编译原理之美相应文章的链接\n",
            "[02:22.500 --> 02:25.100] 那如果你有深入学习的需要的话\n",
            "[02:25.100 --> 02:27.300] 你就能很方便的找到它\n",
            "[02:27.300 --> 02:29.300] 好了,话不多说\n",
            "[02:29.300 --> 02:31.300] 咱们一起开始复习吧\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Transcript file created: test1.srt**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@markdown # **开始运行model** 🚀\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ⚙️\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"zh\" ## 我这里直接写死为中文的\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'srt' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > 要生成哪种类型的字幕文件？\n",
        "\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown ### **Optional: Fine tunning** \n",
        "#@markdown ---\n",
        "temperature = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "title =\"test1\"\n",
        "audio_path = \"/content/downloads/test1.m4a\"\n",
        "audio_path_local = Path(audio_path).resolve()\n",
        "print(\"audio local path:\", audio_path_local)\n",
        "print(\"audio audio_path:\", audio_path)\n",
        "transcription = whisper.transcribe(\n",
        "    whisper_model,\n",
        "    str(audio_path_local),\n",
        "    temperature=temperature,\n",
        "    **args,\n",
        ")\n",
        "\n",
        "# Save output\n",
        "whisper.utils.get_writer(\n",
        "    output_format=output_format,\n",
        "    output_dir=audio_path_local.parent\n",
        ")(\n",
        "    transcription,\n",
        "    title\n",
        ")\n",
        "\n",
        "try:\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('txt', 'vtt', 'srt', 'tsv', 'json'):\n",
        "            transcript_file_name = title + \".\" + ext\n",
        "            display(Markdown(f\"**Transcript file created: {transcript_file_name}**\"))\n",
        "    else:\n",
        "        transcript_file_name = title + \".\" + output_format\n",
        "\n",
        "        display(Markdown(f\"**Transcript file created: {transcript_file_name}**\"))\n",
        "\n",
        "except:\n",
        "    display(Markdown(f\"**Transcript file created: {transcript_local_path}**\"))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.11.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
